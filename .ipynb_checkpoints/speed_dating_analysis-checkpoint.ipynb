{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f429de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3262c3",
   "metadata": {},
   "source": [
    "# .CSV to .DB\n",
    "\n",
    "The below code imports the relevant columns from the .csv file to the .db file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df7e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8378\n"
     ]
    }
   ],
   "source": [
    "# CHANGE TO FUNCTION\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect('speed_dating.db')\n",
    "# Create cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read the CSV as a dataframe\n",
    "df = pd.read_csv('speed_dating.csv', encoding='latin1')\n",
    "\n",
    "# The columns we are interested in\n",
    "columns = ['iid', 'gender', 'pid', 'match', 'dec_o', 'attr_o',\n",
    "           'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'attr3_1',\n",
    "           'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1']\n",
    "\n",
    "# Filter the dataframe to only include the selected columns\n",
    "df_selected = df[columns]\n",
    "\n",
    "# Use Pandas to_sql to insert the DataFrame into the SQLite table\n",
    "df_selected.to_sql('speed_dating', conn, index=False, if_exists='replace')\n",
    "\n",
    "# Create indexes\n",
    "cursor.execute('CREATE INDEX idx_iid ON speed_dating(iid);')\n",
    "cursor.execute('CREATE INDEX idx_pid ON speed_dating(pid);')\n",
    "\n",
    "# Commit changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edbe3e",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "This involves several steps.\n",
    "1) Deleting rows with several NULL values\n",
    "2) Deleting missing (iid, pid), (pid, iid) pairs\n",
    "3) Performing data imputation on missing values\n",
    "\n",
    "\n",
    "## Deleting rows with several NULL values\n",
    "\n",
    "This speed dating dataset revolves around five attributes: attraction, sincerity, intelligence, fun and ambition. \n",
    "\n",
    "For this analysis we are interested in two sets of these attributes. The first, ending in '_o_' (attr_o, sinc_o, etc.), are the ratings an individual (iid) has received from their date partnet (pid).\n",
    "\n",
    "The second, ending in '3_1' (attr3_1, sinc3_1, etc.), are the ratings an individual (iid) gave themnselves before the dating began.\n",
    "\n",
    "For this analysis we will be comparing the importance on each of these attributes on second date success. This means we need all attributes per individual, per date. Data imputation can fill in NULL values if there are only one or two attributes missing per set (the _o_ set and the 3_1 set). However, if several values are missing per set data imputation can be unreliable. For this dataset, I have chosen three or more missing values as being too unreliable. This is so analysis is focused on more quality data. Additionally, there are many rows with one missing attribute, some with two and the rest have all five missing attributes so three felt like an appropriate number.\n",
    "\n",
    "INCLUDE TESTS HERE.\n",
    "\n",
    "The argument for amount of NULL values can be changed when you call the function argument in case you would like to include more data or alternatively more attributes are included in the analysis.\n",
    "\n",
    "\n",
    "## Deleting missing pairs\n",
    "\n",
    "For each date we have an iid and a pid. For each date there are two rows. One is from the perspective of iid (so iid = iid and pid = pid) and the other is from the perspective of pid (so iid = pid and pid = iid). So on the first row (perspective of iid) we see how pid rated iid on these five attributes. On the second row (perspective of pid) we see how iid rated pid on these five attributes. To ensure information is relevant, accurate and complete we must have both perspectives on a particular date. \n",
    "\n",
    "\n",
    "## Data Imputation\n",
    "\n",
    "We now still have rows with one or two missing NULL values for a particular attribute set. For this we will use KNN imputation. Averaging was considered but the reasoning behind not using this method is included in the .readme. \n",
    "\n",
    "INCLUDE CROSS-VALIDATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffee7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE TO TAKE IN ATTRIBUTES\n",
    "# CHANGE TO TAKE IN NUMBER OF MISSING VALUES\n",
    "def delete_null_values(database_path : str):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the attribute columns\n",
    "    # If more attributes are added at a later date\n",
    "    # include them in here\n",
    "    attribute_columns_o = ['attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o']\n",
    "    attribute_columns_3_1 = ['attr3_1', 'sinc3_1', 'intel3_1', 'fun3_1', 'amb3_1']\n",
    "\n",
    "    # Construct the conditions for NULL values in o_columns\n",
    "    conditions_o = []\n",
    "    for column in attribute_columns_o:\n",
    "        conditions_o.append(f\"({column} IS NULL)\")\n",
    "\n",
    "    # Construct the conditions for NULL values in 3_1 columns\n",
    "    conditions_3_1 = []\n",
    "    for column in attribute_columns_3_1:\n",
    "        conditions_3_1.append(f\"({column} IS NULL)\")\n",
    "\n",
    "    # Combine the conditions using AND\n",
    "    # If there are NULL values these will evaluate to 1\n",
    "    # These will add up to 3 or more if there are 3 or \n",
    "    # NULL values in a particular column\n",
    "    condition_query_o = f\"({' + '.join(conditions_o)}) >= 3\"\n",
    "    condition_query_3_1 = f\"({' + '.join(conditions_3_1)}) >= 3\"\n",
    "\n",
    "    # Construct the delete query\n",
    "    delete_query = f\"DELETE FROM speed_dating WHERE ({condition_query_o}) OR ({condition_query_3_1})\"\n",
    "\n",
    "    # Execute the query to find rows to be deleted\n",
    "    cursor.execute(delete_query)\n",
    "\n",
    "    # Get the count of deleted rows\n",
    "    deleted_rows = cursor.rowcount\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Print the number of rows deleted\n",
    "    print(f\"Deleted {deleted_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bbd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_missing_pairs(db_file : str):\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Construct the delete query where there is corresponding\n",
    "    # pid to iid and vice-versa\n",
    "    delete_query = '''\n",
    "    DELETE FROM speed_dating\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM speed_dating AS s2\n",
    "        WHERE speed_dating.iid = s2.pid AND speed_dating.pid = s2.iid)\n",
    "    '''\n",
    "    \n",
    "    # Execute the delete query\n",
    "    cursor.execute(delete_query)\n",
    "    \n",
    "    # Get the count of deleted rows with missing pairs\n",
    "    deleted_missing_pairs = cursor.rowcount\n",
    "    \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    \n",
    "    # Print the number of rows with missing pairs deleted\n",
    "    print(f\"Deleted {deleted_missing_pairs} rows with missing pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7b1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE FUNCTION TO TAKE IN COLUMNS ALSO\n",
    "# CROSS-VALIDATE\n",
    "# TAKE NUMBER OF NEIGHBOURS AS INPUT\n",
    "def data_imputation(db_file : str):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    # Import data into dataframe\n",
    "    df = pd.read_sql_query('SELECT * FROM speed_dating', conn)\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    \n",
    "    # In this case there are only missing values for _o columns\n",
    "    # If they also exist for 3_1 columns simply perform the below\n",
    "    # changing the column ending to 3_1\n",
    "    \n",
    "    # Currently the database size is satisfactory for this approach\n",
    "    # If database size increases consider using batch processing:\n",
    "    # for offset in range(0, total_rows, batch_size):\n",
    "    \n",
    "    # Selecting the columns to impute\n",
    "    # dec_o is included as it is directly linked to attribute ratings\n",
    "    # However, dec_o currently has no missing values\n",
    "    # If there are, these rows should be delet\n",
    "    cols_to_impute = [col for col in df.columns if col.endswith('_o')]\n",
    "    \n",
    "    # Convert the selected columns to numeric\n",
    "    #df[cols_to_impute] = df[cols_to_impute].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Perform KNN imputation\n",
    "    knn_imputer = KNNImputer()\n",
    "    df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[cols_to_impute]), \n",
    "                             columns = cols_to_impute)\n",
    "    \n",
    "    # Update the original DataFrame with the imputed values\n",
    "    df[cols_to_impute] = df_imputed\n",
    "    \n",
    "    # Save the updated data back to the database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql('speed_dating', conn, if_exists = 'replace', index = False)\n",
    "    print('Remaining NULL values have had data imputation performed on them.')\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a62020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 350 rows.\n",
      "Deleted 202 rows with missing pairs.\n",
      "Remaining NULL values have had data imputation performed on them.\n",
      "Operations complete. It took 0.4347 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Running all the data preprocessing functions with timing.\n",
    "start_time = time.time()\n",
    "\n",
    "db_file = 'speed_dating.db'\n",
    "\n",
    "delete_null_values(db_file)\n",
    "delete_missing_pairs(db_file)\n",
    "data_imputation(db_file)\n",
    "\n",
    "end_time = round(time.time() - start_time, 4)\n",
    "\n",
    "print(f'Operations complete. It took {end_time} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
